## 客观指标测试说明（evaluation_objective）

本目录用于设计和记录 **客观指标**，即可以通过程序或明确计算公式得到的量化结果。

这些指标通常用于支撑论文/报告中的图表，例如折线图、柱状图、雷达图等。

客观指标分为两大类：
- **性能指标**：响应时间、吞吐量、稳定性、资源占用等效率相关指标
- **质量指标**：准确率、覆盖率、相关性等效果相关指标

当前计划覆盖的模块：
- `keyword_extractor`（关键词提取）
- `resource_searcher`（资源搜索）
- `recommender`（CBF 推荐）
- `ai_summarizer`（AI 摘要）

---

### 1. 性能指标（Performance Metrics）

性能指标关注系统的效率和稳定性，包括：

- **响应时间 (latency)**：单次调用耗时（秒 / 毫秒）
- **吞吐量 (throughput)**：单位时间内可处理的请求数
- **稳定性**：错误率、超时率
- **资源占用（可选）**：内存、CPU（如使用 `psutil` 等工具）

#### 1.1 性能测试脚本

- `../performance/run_performance_tests.py`：统一入口脚本，调用各模块的测试函数并输出结果

可以按需要补充：
- 测试数据集路径（例如放在 `data/perf_samples/`）
- 每个模块的不同参数配置（例如不同文档数量、不同查询长度等）

#### 1.2 性能指标表格模板

可以在 Excel / Markdown 中使用如下模板（示例）：

##### 1.2.1 关键词提取（`keyword_extractor`）

| 测试编号 | 文档数 | 单文档平均长度(字数) | 总文本大小(MB) | 总耗时(s) | 单文档平均耗时(ms) | 失败次数 | 备注 |
|----------|--------|----------------------|----------------|-----------|--------------------|----------|------|
| KE-01    |        |                      |                |           |                    |          |      |
| KE-02    |        |                      |                |           |                    |          |      |

##### 1.2.2 资源搜索（`resource_searcher`）

| 测试编号 | 查询关键词长度 | 搜索引擎 / 模式 | 返回结果数 | 总耗时(s) | 平均单结果耗时(ms) | 网络错误次数 | 备注 |
|----------|----------------|-----------------|------------|-----------|--------------------|--------------|------|
| RS-01    |                |                 |            |           |                    |              |      |
| RS-02    |                |                 |            |           |                    |              |      |

##### 1.2.3 推荐系统（`recommender`）

| 测试编号 | 用户文档数 | 候选资源数 | 特征维度 | 总耗时(s) | 相似度计算耗时(s) | Top-K 推荐耗时(ms) | 备注 |
|----------|------------|------------|----------|-----------|--------------------|---------------------|------|
| RC-01    |            |            |          |           |                    |                     |      |
| RC-02    |            |            |          |           |                    |                     |      |

##### 1.2.4 AI 摘要（`ai_summarizer`）

| 测试编号 | 资源数 | 单资源平均长度(字数) | 使用模型 | 总耗时(s) | 单资源平均耗时(s) | 失败次数 | 备注 |
|----------|--------|----------------------|----------|-----------|-------------------|----------|------|
| AS-01    |        |                      |          |           |                   |          |      |
| AS-02    |        |                      |          |           |                   |          |      |

---

### 2. 质量指标（Quality Metrics）

质量指标关注系统的准确性和效果，将客观指标按模块划分，并尽量给出明确的 **定义公式**：

- **关键词提取（keyword_extractor）**
  - 关键词覆盖率：\( \text{Coverage} = \frac{\text{命中人工标注核心术语的数量}}{\text{人工标注核心术语总数}} \)
  - 冗余度：\( \text{Redundancy} = \frac{\text{重复或含义高度相近的关键词数}}{\text{关键词总数}} \)
  - 噪声率：\( \text{NoiseRate} = \frac{\text{明显无关/乱码关键词数}}{\text{关键词总数}} \)

- **资源搜索（resource_searcher）**
  - 相关资源比例：\( \text{RelRatio} = \frac{\text{人工标注为"相关"的资源数}}{\text{总资源数}} \)
  - 英文内容比例：\( \text{EnRatio} = \frac{\text{判定为英文的资源数}}{\text{总资源数}} \)

- **推荐系统（recommender）**
  - Top-K 命中率（若有"理想推荐集合"）：\( \text{HitRate@K} = \frac{\text{Top-K 中命中理想集合的资源数}}{K} \)
  - 多样性指标（可选）：基于资源之间内容相似度的平均距离

- **AI 摘要（ai_summarizer）**
  - 可读性得分（如基于长度、句子结构的启发式评分，或人工标注后转成客观分数）
  - 信息覆盖率（是否覆盖标题、主题、关键点等）

---

### 3. 质量指标结果表格模板

#### 3.1 关键词提取指标

| 数据集ID | 文档数 | 人工标注术语数 | 命中术语数 | Coverage | 冗余关键词数 | Redundancy | 噪声关键词数 | NoiseRate | 备注 |
|----------|--------|----------------|------------|----------|--------------|------------|--------------|----------|------|
| DS-KE-01 |        |                |            |          |              |            |              |          |      |
| DS-KE-02 |        |                |            |          |              |            |              |          |      |

#### 3.2 资源搜索指标

| 数据集ID | 查询次数 | 返回资源总数 | 标注为相关的资源数 | RelRatio | 判定为英文的资源数 | EnRatio | 备注 |
|----------|----------|--------------|--------------------|---------|--------------------|--------|------|
| DS-RS-01 |          |              |                    |         |                    |        |      |
| DS-RS-02 |          |              |                    |         |                    |        |      |

#### 3.3 推荐系统指标

| 数据集ID | 用户数 | 候选资源数 | Top-K | 命中资源数 | HitRate@K | 多样性得分（可选） | 备注 |
|----------|--------|------------|-------|------------|-----------|---------------------|------|
| DS-RC-01 |        |            |       |            |           |                     |      |
| DS-RC-02 |        |            |       |            |           |                     |      |

#### 3.4 AI 摘要指标

| 数据集ID | 资源数 | 平均摘要长度(字数) | 可读性平均分 | 信息覆盖率平均分 | 备注 |
|----------|--------|---------------------|--------------|------------------|------|
| DS-AS-01 |        |                     |              |                  |      |
| DS-AS-02 |        |                     |              |                  |      |

---

### 4. 图表建议

收集到上述表格中的数据后，可以绘制：

**性能指标图表：**
- 响应时间变化曲线：如"文档数 vs 响应时间""候选资源数 vs 处理时间"
- 吞吐量对比柱状图：不同配置下的处理速度对比
- 稳定性折线图：错误率/超时率随负载变化的情况

**质量指标图表：**
- 准确率/覆盖率对比柱状图：不同算法配置 / 不同参数设置下的 Coverage / RelRatio / HitRate@K
- 雷达图：将多个指标（如 Coverage / NoiseRate / HitRate / 可读性等）汇总为一个整体对比视图
- 综合性能-质量散点图：横轴为响应时间，纵轴为准确率，展示不同配置的权衡

如果需要，后续生成对应的 `matplotlib` / `seaborn` 绘图脚本。

---

### 5. 需要准备的内容

**性能测试：**
- 一组代表性的测试文档（可以放到 `data/perf_samples/`，例如若干 `.txt`）
- 典型的查询关键词 / 主题描述
- 希望重点对比的场景（例如"10 篇小文档 vs 1000 篇大文档"）

**质量评估：**
- 手工标注的小规模 **金标准数据集**（例如：人工标注的正确关键词集合、理想推荐资源集合、摘要评价表等）
- 每次实验的 **配置说明**（数据集ID、参数设置、模型版本等），以便图表中区分不同实验条件
- 在运行脚本后，将输出结果按上述模板填入表格，以便统一做图
