Source: arXiv
URL: http://arxiv.org/abs/2303.04707v2

==================================================

论文标题: DiM: Distilling Dataset into Generative Model
作者: Kai Wang, Jianyang Gu, Daquan Zhou, Zheng Zhu, Wei Jiang
摘要: Dataset distillation reduces the network training cost by synthesizing small and informative datasets from large-scale ones. Despite the success of the recent dataset distillation algorithms, three drawbacks still limit their wider application: i). the synthetic images perform poorly on large architectures; ii). they need to be re-optimized when the distillation ratio changes; iii). the limited diversity restricts the performance when the distillation ratio is large. In this paper, we propose a novel distillation scheme to \textbf{D}istill information of large train sets \textbf{i}nto generative \textbf{M}odels, named DiM. Specifically, DiM learns to use a generative model to store the information of the target dataset. During the distillation phase, we minimize the differences in logits predicted by a models pool between real and generated images. At the deployment stage, the generative model synthesizes various training samples from random noises on the fly. Due to the simple yet eff...
arXiv链接: http://arxiv.org/abs/2303.04707v2
请访问链接查看完整论文。