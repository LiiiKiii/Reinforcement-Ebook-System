Source: arXiv
URL: http://arxiv.org/abs/1907.02664v2

==================================================

论文标题: Data Encoding for Byzantine-Resilient Distributed Optimization
作者: Deepesh Data, Linqi Song, Suhas Diggavi
摘要: We study distributed optimization in the presence of Byzantine adversaries, where both data and computation are distributed among $m$ worker machines, $t$ of which may be corrupt. The compromised nodes may collaboratively and arbitrarily deviate from their pre-specified programs, and a designated (master) node iteratively computes the model/parameter vector for generalized linear models. In this work, we primarily focus on two iterative algorithms: Proximal Gradient Descent (PGD) and Coordinate Descent (CD). Gradient descent (GD) is a special case of these algorithms. PGD is typically used in the data-parallel setting, where data is partitioned across different samples, whereas, CD is used in the model-parallelism setting, where data is partitioned across the parameter space.   In this paper, we propose a method based on data encoding and error correction over real numbers to combat adversarial attacks. We can tolerate up to $t\leq \lfloor\frac{m-1}{2}\rfloor$ corrupt worker nodes, whi...
arXiv链接: http://arxiv.org/abs/1907.02664v2
请访问链接查看完整论文。