Source: Wikipedia
URL: https://en.wikipedia.org/wiki/natural_language_processing

==================================================

From Wikipedia, the free encyclopedia Processing of natural language by a computer This article has multiple issues. Please help improve it or discuss these issues on the talk page. ( Learn how and when to remove these messages ) This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. Find sources: "Natural language processing" – news · newspapers · books · scholar · JSTOR ( May 2024 ) ( Learn how and when to remove this message ) This article may need to be rewritten to comply with Wikipedia's quality standards. Relevant discussion may be found on the talk page. The talk page may contain suggestions. ( July 2025 ) This article may be in need of reorganization to comply with Wikipedia's layout guidelines. Please help by editing the article to make improvements to the overall structure. ( July 2025 ) ( Learn how and when to remove this message ) ( Learn how and when to remove this message ) Natural language processing ( NLP ) is the processing of natural language information by a computer. NLP is a subfield of computer science and is closely associated with artificial intelligence. NLP is also related to information retrieval , knowledge representation , computational linguistics , and linguistics more broadly. 1 Major processing tasks in an NLP system include: speech recognition , text classification , natural language understanding , and natural language generation. Natural language processing has its roots in the 1950s. 2 Already in 1950, Alan Turing published an article titled " Computing Machinery and Intelligence " which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language. A document parsed into an abstract syntax tree The premise of symbolic NLP is often illustrated using John Searle's Chinese room thought experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts. 1950s : The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem. 1960s : Some notably successful natural language processing systems developed in the 1960s were SHRDLU , a natural language system working in restricted " blocks worlds " with restricted vocabularies, and ELIZA , a simulation of a Rogerian psychotherapy , written by Joseph Weizenbaum between 1964 and 1966. Despite using minimal information about human thought or emotion, ELIZA was able to produce interactions that appeared human-like. When the "patient" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to "My head hurts" with "Why do you say your head hurts?". Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer memory at the time. 5 1970s : During the 1970s, many programmers began to write "conceptual ontologies ", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, the first chatterbots were written (e.g., PARRY ). 1980s : The 1980s and early 1990s mark the heyday of symbolic methods in NLP. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period. 8 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This shift was influenced by increasing computational power (see Moore's law ) and a decline in the dominance of Chomskyan linguistic theories... transformational grammar ), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.