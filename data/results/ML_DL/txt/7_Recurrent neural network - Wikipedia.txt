Source: Wikipedia
URL: https://en.wikipedia.org/wiki/recurrent_neural_networks

==================================================

From Wikipedia, the free encyclopedia (Redirected from Recurrent neural networks ) Class of artificial neural network Part of a series on Machine learning and data mining In artificial neural networks , recurrent neural networks ( RNNs ) are designed for processing sequential data, such as text, speech, and time series , 1 where the order of elements is important. Unlike feedforward neural networks , which process inputs independently, RNNs utilize recurrent connections, where the output of a neuron at one time step is fed back as input to the network at the next time step. This enables RNNs to capture temporal dependencies and patterns within sequences. The fundamental building block of RNN is the recurrent unit , which maintains a hidden state —a form of memory that is updated at each time step based on the current input and the previous hidden state. This feedback mechanism allows the network to learn from past inputs and incorporate that knowledge into its current processing. RNNs have been successfully applied to tasks such as unsegmented, connected handwriting recognition , 2 speech recognition , 3 4 natural language processing , and neural machine translation. 5 6 However, traditional RNNs suffer from the vanishing gradient problem , which limits their ability to learn long-range dependencies. This issue was addressed by the development of the long short-term memory (LSTM) architecture in 1997, making it the standard RNN variant for handling long-term dependencies. Later, gated recurrent units (GRUs) were introduced as a more computationally efficient alternative. In recent years, transformers , which rely on self-attention mechanisms instead of recurrence, have become the dominant architecture for many sequence-processing tasks, particularly in natural language processing, due to their superior handling of long-range dependencies and greater parallelizability. Nevertheless, RNNs remain relevant for applications where computational efficiency, real-time processing, or the inherent sequential nature of data is crucial. One origin of RNN was neuroscience. The word "recurrent" is used to describe loop-like structures in anatomy. In 1901, Cajal observed "recurrent semicircles" in the cerebellar cortex formed by parallel fiber , Purkinje cells , and granule cells. 7 8 In 1933, Lorente de Nó discovered "recurrent, reciprocal connections" by Golgi's method , and proposed that excitatory loops explain certain aspects of the vestibulo-ocular reflex. 9 10 During 1940s, multiple people proposed the existence of feedback in the brain, which was a contrast to the previous understanding of the neural system as a purely feedforward structure. Hebb considered "reverberating circuit" as an explanation for short-term memory. 11 The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles. The current activity of such networks can be affected by activity indefinitely far in the past. 12 They were both interested in closed loops as possible explanations for e.g. epilepsy and causalgia. 13 14 Recurrent inhibition was proposed in 1946 as a negative feedback mechanism in motor control. Neural feedback loops were a common topic of discussion at the Macy conferences. 15 See 16 for an extensive review of recurrent neural network models in neuroscience. A close-loop cross-coupled perceptron network 17 403, Fig. 47 Frank Rosenblatt in 1960 published "close-loop cross-coupled perceptrons", which are 3-layered perceptron networks whose middle layer contains recurrent connections that change by a Hebbian learning rule. 18 73–75 Later, in Principles of Neurodynamics (1961), he described "closed-loop cross-coupled" and "back-coupled" perceptron networks, and made theoretical and experimental studies for Hebbian learning in these networks, 17 Chapter 19, 21 and noted that a fully cross-coupled perceptron network is equivalent to an infinitely deep feedforward network. 17 Section 19.11 Similar networks were published by Kaoru Nakano in 1971, 19 20 Shun'ichi Amari in 1972, 21 and William A. Little de in 1974, 22 who was acknowledged by Hopfield in his 1982 paper. Another origin of RNN was statistical mechanics. The Ising model was developed by Wilhelm Lenz 23 and Ernst Ising 24 in the 1920s 25 as a simple statistical mechanical model of magnets at equilibrium. Glauber in 1963 studied the Ising model evolving in time, as a process towards equilibrium ( Glauber dynamics ), adding in the component of time. 26 The Sherrington–Kirkpatrick model of spin glass, published in 1975, 27 is the Hopfield network with random initialization. Sherrington and Kirkpatrick found that it is highly likely for the energy function of the SK model to have many local minima. In the 1982 paper, Hopfield applied this recently developed theory to study the Hopfield network with binary activation functions. 28 In a 1984 paper he extended this to continuous activation functions.